{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/nix-shell.Pm1jxz/ipykernel_10079/3809503550.py:24: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image, ImageFile\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.models as models\n",
    "import time\n",
    "import torchinfo\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from pyutils.lazynoisedataset import LazyNoiseDataset\n",
    "from pyutils.distdataset import DistDataset\n",
    "\n",
    "\n",
    "from tqdm.autonotebook import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Select device\n",
    "device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "#elif torch.xpu.is_available():\n",
    "#    device = torch.device(\"xpu\")\n",
    "    \n",
    "\n",
    "\n",
    "print(\"Device: {}\".format(device))\n",
    "\n",
    "\n",
    "# utils\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7727 datapoints from  cheXpert/cxp_cxrs000.pt\n",
      "7660 datapoints from  cheXpert/cxp_cxrs001.pt\n",
      "7753 datapoints from  cheXpert/cxp_cxrs002.pt\n",
      "7617 datapoints from  cheXpert/cxp_cxrs003.pt\n",
      "7728 datapoints from  cheXpert/cxp_cxrs004.pt\n",
      "7657 datapoints from  cheXpert/cxp_cxrs005.pt\n",
      "7756 datapoints from  cheXpert/cxp_cxrs006.pt\n",
      "7712 datapoints from  cheXpert/cxp_cxrs007.pt\n",
      "7669 datapoints from  cheXpert/cxp_cxrs008.pt\n",
      "7626 datapoints from  cheXpert/cxp_cxrs009.pt\n",
      "7666 datapoints from  cheXpert/cxp_cxrs010.pt\n",
      "7709 datapoints from  cheXpert/cxp_cxrs011.pt\n",
      "7754 datapoints from  cheXpert/cxp_cxrs012.pt\n",
      "7678 datapoints from  cheXpert/cxp_cxrs013.pt\n",
      "8039 datapoints from  cheXpert/cxp_cxrs014.pt\n",
      "9512 datapoints from  cheXpert/cxp_cxrs015.pt\n",
      "9506 datapoints from  cheXpert/cxp_cxrs016.pt\n",
      "9508 datapoints from  cheXpert/cxp_cxrs017.pt\n",
      "9430 datapoints from  cheXpert/cxp_cxrs018.pt\n",
      "9400 datapoints from  cheXpert/cxp_cxrs019.pt\n",
      "9376 datapoints from  cheXpert/cxp_cxrs020.pt\n",
      "9314 datapoints from  cheXpert/cxp_cxrs021.pt\n",
      "9230 datapoints from  cheXpert/cxp_cxrs022.pt\n"
     ]
    }
   ],
   "source": [
    "# my utils\n",
    "from pyutils.lazynoisedataset import LazyNoiseDataset\n",
    "from pyutils.distdataset import DistDataset, SplitDataset\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# loading dataset\n",
    "cxr_images = DistDataset(\"cheXpert/cxp_cxrs{:03}.pt\")\n",
    "noise_dataset = LazyNoiseDataset(cxr_images)\n",
    "#noise_dataset = LazyNoiseDataset.from_distdataset_pickle(\"pyutils/pickles/cxrdataset.pickle\")\n",
    "\n",
    "train_noise_dataset = SplitDataset(noise_dataset, split_end=len(noise_dataset) * 0.8)\n",
    "test_noise_dataset  = SplitDataset(noise_dataset, split_start=len(noise_dataset) * 0.8)\n",
    "\n",
    "\n",
    "train_noise_dataloader  = DataLoader(train_noise_dataset, batch_size=32, num_workers=2)\n",
    "test_noise_dataloader   = DataLoader(test_noise_dataset, batch_size=32, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx=0: loafing 0th datapoint from cheXpert/cxp_cxrs000.pt\n",
      "torch.Size([1, 320, 320])\n",
      "idx=0: loafing 0th datapoint from cheXpert/cxp_cxrs000.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Autoencoder                              [1, 320, 320]             --\n",
       "├─Sequential: 1-1                        [32, 80, 80]              --\n",
       "│    └─Conv2d: 2-1                       [16, 320, 320]            160\n",
       "│    └─ReLU: 2-2                         [16, 320, 320]            --\n",
       "│    └─MaxPool2d: 2-3                    [16, 160, 160]            --\n",
       "│    └─Conv2d: 2-4                       [32, 160, 160]            4,640\n",
       "│    └─ReLU: 2-5                         [32, 160, 160]            --\n",
       "│    └─MaxPool2d: 2-6                    [32, 80, 80]              --\n",
       "├─Sequential: 1-2                        [1, 320, 320]             --\n",
       "│    └─ConvTranspose2d: 2-7              [16, 160, 160]            4,624\n",
       "│    └─ReLU: 2-8                         [16, 160, 160]            --\n",
       "│    └─ConvTranspose2d: 2-9              [1, 320, 320]             145\n",
       "│    └─Sigmoid: 2-10                     [1, 320, 320]             --\n",
       "==========================================================================================\n",
       "Total params: 9,569\n",
       "Trainable params: 9,569\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 36.46\n",
       "==========================================================================================\n",
       "Input size (MB): 0.41\n",
       "Forward/backward pass size (MB): 23.76\n",
       "Params size (MB): 0.04\n",
       "Estimated Total Size (MB): 24.20\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the autoencoder architecture\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        ).to(device)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(32, 16,\n",
    "                               kernel_size=3,\n",
    "                               stride=2,\n",
    "                               padding=1,\n",
    "                               output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 1,\n",
    "                               kernel_size=3, # unet uses 2?\n",
    "                               stride=2,\n",
    "                               padding=1,\n",
    "                               output_padding=1),\n",
    "            nn.Sigmoid()\n",
    "        ).to(device)\n",
    "\n",
    "\n",
    "    def forward(self, x, residual=False):\n",
    "        if residual:\n",
    "            return self.forward_residual(x)\n",
    "        else:\n",
    "            return self.forward_non_residual(x)\n",
    "\n",
    "    def forward_residual(self, x):\n",
    "        x = x.to(device)\n",
    "        r = x.clone()\n",
    "        r = self.forward_non_residual(r)\n",
    "        return x + r\n",
    "\n",
    "\n",
    "    def forward_non_residual(self, x):\n",
    "        x = x.to(device)\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "m = Autoencoder()\n",
    "m = m.to(device)\n",
    "torchinfo.summary(m, noise_dataset[0][0].shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx=0: loafing 0th datapoint from cheXpert/cxp_cxrs000.ptidx=32: loafing 32th datapoint from cheXpert/cxp_cxrs000.pt\n",
      "\n",
      "idx=1: loafing 1th datapoint from cheXpert/cxp_cxrs000.pt\n",
      "idx=33: loafing 33th datapoint from cheXpert/cxp_cxrs000.pt\n",
      "idx=2: loafing 2th datapoint from cheXpert/cxp_cxrs000.pt\n",
      "idx=34: loafing 34th datapoint from cheXpert/cxp_cxrs000.pt\n",
      "idx=35: loafing 35th datapoint from cheXpert/cxp_cxrs000.pt\n",
      "idx=3: loafing 3th datapoint from cheXpert/cxp_cxrs000.pt\n",
      "idx=36: loafing 36th datapoint from cheXpert/cxp_cxrs000.pt\n",
      "idx=4: loafing 4th datapoint from cheXpert/cxp_cxrs000.pt\n",
      "idx=5: loafing 5th datapoint from cheXpert/cxp_cxrs000.ptidx=37: loafing 37th datapoint from cheXpert/cxp_cxrs000.pt\n",
      "\n",
      "idx=6: loafing 6th datapoint from cheXpert/cxp_cxrs000.pt\n",
      "idx=38: loafing 38th datapoint from cheXpert/cxp_cxrs000.pt\n",
      "idx=7: loafing 7th datapoint from cheXpert/cxp_cxrs000.ptidx=39: loafing 39th datapoint from cheXpert/cxp_cxrs000.pt\n",
      "\n",
      "idx=40: loafing 40th datapoint from cheXpert/cxp_cxrs000.pt\n",
      "idx=8: loafing 8th datapoint from cheXpert/cxp_cxrs000.pt\n",
      "idx=41: loafing 41th datapoint from cheXpert/cxp_cxrs000.pt\n",
      "idx=9: loafing 9th datapoint from cheXpert/cxp_cxrs000.pt\n",
      "idx=42: loafing 42th datapoint from cheXpert/cxp_cxrs000.pt\n",
      "idx=10: loafing 10th datapoint from cheXpert/cxp_cxrs000.pt\n",
      "idx=43: loafing 43th datapoint from cheXpert/cxp_cxrs000.pt\n",
      "idx=11: loafing 11th datapoint from cheXpert/cxp_cxrs000.pt\n",
      "idx=44: loafing 44th datapoint from cheXpert/cxp_cxrs000.ptidx=12: loafing 12th datapoint from cheXpert/cxp_cxrs000.pt\n",
      "\n",
      "idx=45: loafing 45th datapoint from cheXpert/cxp_cxrs000.ptidx=13: loafing 13th datapoint from cheXpert/cxp_cxrs000.pt\n",
      "\n",
      "idx=46: loafing 46th datapoint from cheXpert/cxp_cxrs000.pt\n",
      "idx=14: loafing 14th datapoint from cheXpert/cxp_cxrs000.pt\n",
      "idx=47: loafing 47th datapoint from cheXpert/cxp_cxrs000.pt\n",
      "idx=15: loafing 15th datapoint from cheXpert/cxp_cxrs000.pt\n",
      "idx=48: loafing 48th datapoint from cheXpert/cxp_cxrs000.pt\n",
      "idx=16: loafing 16th datapoint from cheXpert/cxp_cxrs000.pt\n",
      "idx=49: loafing 49th datapoint from cheXpert/cxp_cxrs000.pt\n",
      "idx=17: loafing 17th datapoint from cheXpert/cxp_cxrs000.pt\n",
      "idx=18: loafing 18th datapoint from cheXpert/cxp_cxrs000.pt\n",
      "idx=50: loafing 50th datapoint from cheXpert/cxp_cxrs000.pt\n",
      "idx=51: loafing 51th datapoint from cheXpert/cxp_cxrs000.pt\n",
      "idx=19: loafing 19th datapoint from cheXpert/cxp_cxrs000.pt\n",
      "idx=52: loafing 52th datapoint from cheXpert/cxp_cxrs000.pt\n",
      "idx=20: loafing 20th datapoint from cheXpert/cxp_cxrs000.pt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 29\u001b[0m\n\u001b[1;32m     25\u001b[0m progress_log \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m%\u001b[39m (NUM_EPOCHS \u001b[38;5;241m/\u001b[39m NUM_TESTING_EPOCHS) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m epoch \u001b[38;5;241m==\u001b[39m NUM_EPOCHS \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     27\u001b[0m train_loss_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m train_noise_dataloader:\n\u001b[1;32m     30\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     31\u001b[0m     output \u001b[38;5;241m=\u001b[39m m(x_batch)\n",
      "File \u001b[0;32m/nix/store/zmim4ghaaa5zmnw8vxrvz4b768bz9sgy-python3-3.10.13-env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/nix/store/zmim4ghaaa5zmnw8vxrvz4b768bz9sgy-python3-3.10.13-env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1328\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1327\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1328\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1331\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/nix/store/zmim4ghaaa5zmnw8vxrvz4b768bz9sgy-python3-3.10.13-env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1294\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1290\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1291\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1293\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1294\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1295\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1296\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/nix/store/zmim4ghaaa5zmnw8vxrvz4b768bz9sgy-python3-3.10.13-env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1132\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1132\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1133\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1134\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m/nix/store/h31x3zrwrws1c3392919br8g9isvhh4x-python3-3.10.13/lib/python3.10/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[1;32m    112\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "File \u001b[0;32m/nix/store/h31x3zrwrws1c3392919br8g9isvhh4x-python3-3.10.13/lib/python3.10/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nix/store/h31x3zrwrws1c3392919br8g9isvhh4x-python3-3.10.13/lib/python3.10/multiprocessing/connection.py:424\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 424\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m/nix/store/h31x3zrwrws1c3392919br8g9isvhh4x-python3-3.10.13/lib/python3.10/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    928\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m    933\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "File \u001b[0;32m/nix/store/h31x3zrwrws1c3392919br8g9isvhh4x-python3-3.10.13/lib/python3.10/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Options\n",
    "NUM_EPOCHS = 10\n",
    "NUM_IMG_EXAMPLES = 5\n",
    "NUM_TESTING_EPOCHS = 10 # in addition to last\n",
    "\n",
    "# Time and logging\n",
    "start_time = time.time()\n",
    "last_epoch_time = time.time()# to be updated\n",
    "\n",
    "training_loss_hist = []\n",
    "testing_loss_hist = []\n",
    "testing_epoch_is = []\n",
    "\n",
    "\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(m.parameters(), lr=0.001)\n",
    "\n",
    "# Train the autoencoder\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    m.train()\n",
    "\n",
    "    progress_log = epoch % (NUM_EPOCHS / NUM_TESTING_EPOCHS) == 0 or epoch == NUM_EPOCHS - 1\n",
    "\n",
    "    train_loss_acc = 0.0\n",
    "\n",
    "    for x_batch, y_batch in train_noise_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        output = m(x_batch)\n",
    "        loss = criterion(y_batch.to(device), output)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_acc += loss.item()\n",
    "\n",
    "    train_loss = train_loss_acc / len(train_noise_dataloader)\n",
    "    training_loss_hist.append(train_loss)\n",
    "\n",
    "\n",
    "    epoch_time = time.time() - last_epoch_time\n",
    "    last_epoch_time = time.time()\n",
    "\n",
    "    if progress_log:\n",
    "        testing_epoch_is.append(epoch)\n",
    "        m.eval()\n",
    "\n",
    "        test_loss_acc = 0.0\n",
    "        for x_test_batch, y_test_batch in test_noise_dataloader:\n",
    "            output = m(x_test_batch.to(device))\n",
    "            loss = criterion(y_test_batch.to(device), output)\n",
    "            test_loss_acc += loss.item()\n",
    "\n",
    "        test_loss = test_loss_acc / len(test_noise_dataloader)\n",
    "        testing_loss_hist.append(test_loss)\n",
    "\n",
    "\n",
    "        fig, axs = plt.subplots(NUM_IMG_EXAMPLES, 3, figsize=(8, 8))\n",
    "\n",
    "        axs[0,0].set_title(\"noisy\")\n",
    "        axs[0,1].set_title(\"real\")\n",
    "        axs[0,2].set_title(\"restored\")\n",
    "\n",
    "        for i in range(NUM_IMG_EXAMPLES):\n",
    "            x, y = test_noise_dataloader.dataset[i]\n",
    "            o = m(x.to(device))\n",
    "\n",
    "            axs[i,0].set_axis_off()\n",
    "            axs[i,1].set_axis_off()\n",
    "            axs[i,2].set_axis_off()\n",
    "\n",
    "            axs[i,0].imshow(x.cpu().reshape((28,28)), cmap='gray')\n",
    "            axs[i,1].imshow(y.cpu().reshape((28,28)), cmap='gray')\n",
    "            axs[i,2].imshow(o.cpu().detach().numpy().reshape((28,28)), cmap='gray')\n",
    "\n",
    "        plt.show()\n",
    "        print(\n",
    "\"\"\"\n",
    "Epoch [{}/{}]\n",
    "Loss - Train: {:.4f}   Test: {:.4f}\n",
    "Time - since start: {:.1f}   this epoch: {:.1f}\n",
    "(note: 'since start' does include total testing...)\n",
    "\"\"\".format(epoch+1, NUM_EPOCHS, train_loss, test_loss, time.time() - start_time, epoch_time))\n",
    "\n",
    "\n",
    "plt.plot( [i+1 for i in range(NUM_EPOCHS)], training_loss_hist, \"b:o\", label=\"Train Loss\")\n",
    "plt.plot([i+1 for i in testing_epoch_is], testing_loss_hist,  \"r:o\", label=\"Test Loss\")\n",
    "plt.title = \"Loss over time\"\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "torch.save(m.state_dict(), \"models/last_model\")\n",
    "torch.save(m.state_dict(), \"models/all/[{}]-{}params\".format(datetime.datetime.now().strftime(\"%d%b-%H.%M\"), count_parameters(m)))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
