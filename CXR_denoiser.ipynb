{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image, ImageFile\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.models as models\n",
    "import time\n",
    "import torchinfo\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset\n",
    "import time\n",
    "import datetime\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "from tqdm.autonotebook import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Select device\n",
    "device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "#elif torch.xpu.is_available():\n",
    "#   device = torch.device(\"xpu\")\n",
    "    \n",
    "\n",
    "\n",
    "print(\"Device: {}\".format(device))\n",
    "\n",
    "\n",
    "# utils\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[cxr_imgs pickle dir '/home/morris/Workspace/Python/xjobb/data/cheXpert/cxr_imgs_dataset.pickle']\n",
      "loading pickle...\n",
      "noise dataset len:  191027\n",
      "  train noise dataset len:  100\n",
      "  test  noise dataset len:  25\n"
     ]
    }
   ],
   "source": [
    "# my utils\n",
    "\n",
    "\n",
    "#import data\n",
    "#import data.datahandlers\n",
    "#print(data.datahandlers)\n",
    "\n",
    "#from pyutils.distdataset import DistDataset, SplitDataset\n",
    "#from pyutils.lazynoisedataset import LazyNoiseDataset\n",
    "import data.datahandlers as datahandlers\n",
    "\n",
    "\n",
    "from data.datahandlers import LazyNoiseDataset, DistDataset, SplitDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "\n",
    "# loading dataset\n",
    "cxr_imgs_dataset: DistDataset = None\n",
    "\n",
    "# pickle!\n",
    "\n",
    "\n",
    "\n",
    "dirname = os.getcwd()\n",
    "cxr_imgs_pickle_dir = dirname + \"/data/cheXpert/cxr_imgs_dataset.pickle\"\n",
    "print(\"[cxr_imgs pickle dir '{}']\".format(cxr_imgs_pickle_dir))\n",
    "\n",
    "if not os.path.isfile(cxr_imgs_pickle_dir):\n",
    "    print(\"building new dataset - no pickle found\")\n",
    "    cxr_imgs_dataset = DistDataset( dirname + \"/data/cheXpert/cxr_imgs{:03}.pt\")\n",
    "    \n",
    "    with open(cxr_imgs_pickle_dir, \"wb\") as f:\n",
    "        pickle.dump(cxr_imgs_dataset, f)\n",
    "        print(\"  dumped pickle!\" )\n",
    "\n",
    "\n",
    "\n",
    "with open(cxr_imgs_pickle_dir, \"rb\") as f:\n",
    "    print(\"loading pickle...\")\n",
    "    cxr_imgs_dataset = pickle.load(f)\n",
    "    pass\n",
    "#try: \n",
    "#    print(\"trying to unpickle dataset\")\n",
    "#    with open(\"data/cheXpert/cxr_imgs_dataset.pickle\", \"rb\") as f:\n",
    "#        cxr_imgs_dataset = pickle.load(f)\n",
    "#except:\n",
    "#    print(\"building new dataset - unpickling failed\")\n",
    "#    cxr_imgs_dataset = DistDataset(\"../cheXpert/cxp_cxrs{:03}.pt\")\n",
    "#\n",
    "#    with open(\"data/cheXpert/cxr_imgs_dataset.pickle\", \"wb\") as f:\n",
    "#        pickle.dump(cxr_imgs_dataset, f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "noise_dataset = LazyNoiseDataset(cxr_imgs_dataset)\n",
    "#noise_dataset = LazyNoiseDataset.from_distdataset_pickle(\"pyutils/pickles/cxrdataset.pickle\")\n",
    "\n",
    "print(\"noise dataset len: \", len(noise_dataset))\n",
    "\n",
    "train_noise_dataset = SplitDataset(noise_dataset, split_end=100)\n",
    "test_noise_dataset  = SplitDataset(noise_dataset, split_start=100, split_end=125)\n",
    "\n",
    "print(\"  train noise dataset len: \", len(train_noise_dataset))\n",
    "print(\"  test  noise dataset len: \", len(test_noise_dataset))\n",
    "\n",
    "\n",
    "\n",
    "#train_noise_dataset = SplitDataset(noise_dataset, split_end=len(noise_dataset) * 0.8)\n",
    "#test_noise_dataset  = SplitDataset(noise_dataset, split_start=len(noise_dataset) * 0.8)\n",
    "\n",
    "\n",
    "train_noise_dataloader  = DataLoader(train_noise_dataset, batch_size=32, num_workers=2)\n",
    "test_noise_dataloader   = DataLoader(test_noise_dataset, batch_size=32, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nix/store/zmim4ghaaa5zmnw8vxrvz4b768bz9sgy-python3-3.10.13-env/lib/python3.10/site-packages/torchinfo/torchinfo.py:477: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  action_fn=lambda data: sys.getsizeof(data.storage()),\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "UNet                                     [1, 1, 320, 320]          --\n",
       "├─ModuleList: 1-7                        --                        (recursive)\n",
       "│    └─DoubleConv: 2-1                   [1, 64, 320, 320]         --\n",
       "│    │    └─Sequential: 3-1              [1, 64, 320, 320]         37,824\n",
       "├─MaxPool2d: 1-2                         [1, 64, 160, 160]         --\n",
       "├─ModuleList: 1-7                        --                        (recursive)\n",
       "│    └─DoubleConv: 2-2                   [1, 128, 160, 160]        --\n",
       "│    │    └─Sequential: 3-2              [1, 128, 160, 160]        221,952\n",
       "├─MaxPool2d: 1-4                         [1, 128, 80, 80]          --\n",
       "├─ModuleList: 1-7                        --                        (recursive)\n",
       "│    └─DoubleConv: 2-3                   [1, 256, 80, 80]          --\n",
       "│    │    └─Sequential: 3-3              [1, 256, 80, 80]          886,272\n",
       "├─MaxPool2d: 1-6                         [1, 256, 40, 40]          --\n",
       "├─ModuleList: 1-7                        --                        (recursive)\n",
       "│    └─DoubleConv: 2-4                   [1, 512, 40, 40]          --\n",
       "│    │    └─Sequential: 3-4              [1, 512, 40, 40]          3,542,016\n",
       "├─MaxPool2d: 1-8                         [1, 512, 20, 20]          --\n",
       "├─DoubleConv: 1-9                        [1, 1024, 20, 20]         --\n",
       "│    └─Sequential: 2-5                   [1, 1024, 20, 20]         --\n",
       "│    │    └─Conv2d: 3-5                  [1, 1024, 20, 20]         4,719,616\n",
       "│    │    └─BatchNorm2d: 3-6             [1, 1024, 20, 20]         2,048\n",
       "│    │    └─ReLU: 3-7                    [1, 1024, 20, 20]         --\n",
       "│    │    └─Conv2d: 3-8                  [1, 1024, 20, 20]         9,438,208\n",
       "│    │    └─BatchNorm2d: 3-9             [1, 1024, 20, 20]         2,048\n",
       "│    │    └─ReLU: 3-10                   [1, 1024, 20, 20]         --\n",
       "├─ModuleList: 1-16                       --                        (recursive)\n",
       "│    └─ConvTranspose2d: 2-6              [1, 512, 40, 40]          2,097,664\n",
       "├─ModuleList: 1-17                       --                        (recursive)\n",
       "│    └─DoubleConv: 2-7                   [1, 512, 40, 40]          --\n",
       "│    │    └─Sequential: 3-11             [1, 512, 40, 40]          7,080,960\n",
       "├─ModuleList: 1-16                       --                        (recursive)\n",
       "│    └─ConvTranspose2d: 2-8              [1, 256, 80, 80]          524,544\n",
       "├─ModuleList: 1-17                       --                        (recursive)\n",
       "│    └─DoubleConv: 2-9                   [1, 256, 80, 80]          --\n",
       "│    │    └─Sequential: 3-12             [1, 256, 80, 80]          1,771,008\n",
       "├─ModuleList: 1-16                       --                        (recursive)\n",
       "│    └─ConvTranspose2d: 2-10             [1, 128, 160, 160]        131,200\n",
       "├─ModuleList: 1-17                       --                        (recursive)\n",
       "│    └─DoubleConv: 2-11                  [1, 128, 160, 160]        --\n",
       "│    │    └─Sequential: 3-13             [1, 128, 160, 160]        443,136\n",
       "├─ModuleList: 1-16                       --                        (recursive)\n",
       "│    └─ConvTranspose2d: 2-12             [1, 64, 320, 320]         32,832\n",
       "├─ModuleList: 1-17                       --                        (recursive)\n",
       "│    └─DoubleConv: 2-13                  [1, 64, 320, 320]         --\n",
       "│    │    └─Sequential: 3-14             [1, 64, 320, 320]         110,976\n",
       "├─Conv2d: 1-18                           [1, 1, 320, 320]          65\n",
       "==========================================================================================\n",
       "Total params: 31,042,369\n",
       "Trainable params: 31,042,369\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 85.27\n",
       "==========================================================================================\n",
       "Input size (MB): 0.41\n",
       "Forward/backward pass size (MB): 898.66\n",
       "Params size (MB): 124.17\n",
       "Estimated Total Size (MB): 1023.24\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyutils.myUNet import UNet\n",
    "\n",
    "m = UNet()\n",
    "m = m.to(device)\n",
    "torchinfo.summary(m, (1,1,320,320))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the autoencoder architecture\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        ).to(device)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(32, 16,\n",
    "                               kernel_size=3,\n",
    "                               stride=2,\n",
    "                               padding=1,\n",
    "                               output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 1,\n",
    "                               kernel_size=3, # unet uses 2?\n",
    "                               stride=2,\n",
    "                               padding=1,\n",
    "                               output_padding=1),\n",
    "            nn.Sigmoid()\n",
    "        ).to(device)\n",
    "\n",
    "\n",
    "    def forward(self, x, residual=True):\n",
    "        if residual:\n",
    "            return self.forward_residual(x)\n",
    "        else:\n",
    "            return self.forward_non_residual(x)\n",
    "\n",
    "    def forward_residual(self, x):\n",
    "        x = x.to(device)\n",
    "        r = x.clone()\n",
    "        r = self.forward_non_residual(r)\n",
    "        return x + r\n",
    "\n",
    "\n",
    "    def forward_non_residual(self, x):\n",
    "        x = x.to(device)\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "#m = Autoencoder()\n",
    "#m = m.to(device)\n",
    "#torchinfo.summary(m, noise_dataset[0][0].shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c67e82c5d200475fbd1e534cf54db95f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Options\n",
    "NUM_EPOCHS = 10\n",
    "NUM_IMG_EXAMPLES = 5\n",
    "NUM_TESTING_EPOCHS = 10 # in addition to last\n",
    "\n",
    "# Time and logging\n",
    "start_time = time.time()\n",
    "last_epoch_time = time.time()# to be updated\n",
    "\n",
    "training_loss_hist = []\n",
    "testing_loss_hist = []\n",
    "testing_epoch_is = []\n",
    "\n",
    "\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(m.parameters(), lr=0.001)\n",
    "\n",
    "# Train the autoencoder\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    m.train()\n",
    "\n",
    "    progress_log = epoch % (NUM_EPOCHS / NUM_TESTING_EPOCHS) == 0 or epoch == NUM_EPOCHS - 1\n",
    "\n",
    "    train_loss_acc = 0.0\n",
    "\n",
    "    for x_batch, y_batch in tqdm(train_noise_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        output = m(x_batch)\n",
    "        loss = criterion(y_batch.to(device), output)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_acc += loss.item()\n",
    "\n",
    "    train_loss = train_loss_acc / len(train_noise_dataloader)\n",
    "    training_loss_hist.append(train_loss)\n",
    "\n",
    "\n",
    "    epoch_time = time.time() - last_epoch_time\n",
    "    last_epoch_time = time.time()\n",
    "\n",
    "    if progress_log:\n",
    "        testing_epoch_is.append(epoch)\n",
    "        m.eval()\n",
    "\n",
    "        test_loss_acc = 0.0\n",
    "        for x_test_batch, y_test_batch in test_noise_dataloader:\n",
    "            output = m(x_test_batch.to(device))\n",
    "            loss = criterion(y_test_batch.to(device), output)\n",
    "            test_loss_acc += loss.item()\n",
    "\n",
    "        test_loss = test_loss_acc / len(test_noise_dataloader)\n",
    "        testing_loss_hist.append(test_loss)\n",
    "\n",
    "\n",
    "        fig, axs = plt.subplots(NUM_IMG_EXAMPLES, 3, figsize=(8, 8))\n",
    "\n",
    "        axs[0,0].set_title(\"noisy\")\n",
    "        axs[0,1].set_title(\"real\")\n",
    "        axs[0,2].set_title(\"restored\")\n",
    "\n",
    "        for i in range(NUM_IMG_EXAMPLES):\n",
    "            print(type(test_noise_dataloader.dataset[i]))\n",
    "            x, y = test_noise_dataloader.dataset[i]\n",
    "            o = m(x.to(device))\n",
    "\n",
    "            axs[i,0].set_axis_off()\n",
    "            axs[i,1].set_axis_off()\n",
    "            axs[i,2].set_axis_off()\n",
    "\n",
    "            axs[i,0].imshow(x.cpu().squeeze(), cmap='gray')\n",
    "            axs[i,1].imshow(y.cpu().squeeze(), cmap='gray')\n",
    "            axs[i,2].imshow(o.cpu().detach().numpy().squeeze(), cmap='gray')\n",
    "\n",
    "        plt.show()\n",
    "        print(\n",
    "\"\"\"\n",
    "Epoch [{}/{}]\n",
    "Loss - Train: {:.4f}   Test: {:.4f}\n",
    "Time - since start: {:.1f}   this epoch: {:.1f}\n",
    "(note: 'since start' does include total testing...)\n",
    "\"\"\".format(epoch+1, NUM_EPOCHS, train_loss, test_loss, time.time() - start_time, epoch_time))\n",
    "\n",
    "\n",
    "plt.plot( [i+1 for i in range(NUM_EPOCHS)], training_loss_hist, \"b:o\", label=\"Train Loss\")\n",
    "plt.plot([i+1 for i in testing_epoch_is], testing_loss_hist,  \"r:o\", label=\"Test Loss\")\n",
    "plt.title = \"Loss over time\"\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "torch.save(m.state_dict(), \"models/last_model\")\n",
    "torch.save(m.state_dict(), \"models/all/[{}]-{}params\".format(datetime.datetime.now().strftime(\"%d%b-%H.%M\"), count_parameters(m)))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
